[project]
name = "mmu-rag"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "aiohttp>=3.12.15",
    "botocore>=1.40.35",
    "datasets>=4.1.0",
    "fastapi[standard]>=0.116.1",
    "ipykernel>=6.30.1",
    "jsonlines>=4.0.0",
    "langchain-aws>=0.2.33",
    "langchain-core>=0.3.76",
    "langchain-openai>=0.3.29",
    "langgraph>=0.6.8",
    "litellm>=1.75.3",
    "matplotlib>=3.10.7",
    "nltk>=3.9.1",
    "numpy>=1.26.4",
    "openai>=1.99.1",
    "pandas>=2.3.2",
    "posthog>=6.7.5",
    "pydantic>=2.11.7",
    "seaborn>=0.13.2",
    "sentence-transformers>=3.0.0",
    "structlog>=25.4.0",
    "tiktoken>=0.8.0",
    "transformers>=4.51", # minimum version for Qwen reranker
]

[project.optional-dependencies]
# uv add torch --optional vllm
# uv sync --extra vllm
# or | uv sync --all-extras
# required for local llm_servers
# flashinfer-python 0.3.1 has issues with our installation, pin to 0.2.2
vllm = ["vllm==0.10.2", "flashinfer-python==0.2.2", "numpy==1.26.4"]
# required for query_complexity.py
classifier = ["pip", "spacy==3.8.7", "scikit-learn", "joblib"]
eval = [
    "apscheduler>=3.11.0",
    "cryptography>=46.0.1",
    "fastapi-sso>=0.18.0",
    "deepeval>=3.6.2",
    "rouge-score>=0.1.2",
    "bert-score>=0.3.13",
]
# required for scripts/proxy.py
proxy = ["llm-proxy-ondemand>=0.1.3", "awscli>=1.42.42"]

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]

[dependency-groups]
dev = ["autopep8>=2.3.2"]

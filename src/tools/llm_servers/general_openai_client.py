"""
Client for interacting with OpenAI-compatible API using the official OpenAI Python client.
"""
import os
import json
import time
from typing import Dict, Optional, Tuple, Any, List, AsyncGenerator
from openai import OpenAI, AsyncOpenAI, APIError, APIConnectionError, RateLimitError
from openai.types.chat import ChatCompletionMessageParam
from datetime import datetime

from tools.llm_servers.llm_interface import LLMInterface
from tools.llm_servers.sglang_types import CustomChatCompletionChunk
from tools.logging_utils import get_logger
from tools.path_utils import get_data_dir
from tools.retry_utils import retry


class GeneralOpenAIClient(LLMInterface):
    """Client for interacting with OpenAI-compatible API."""

    def __init__(
        self,
        api_base: str,
        api_key: Optional[str] = None,
        max_retries: int = 5,
        timeout: float = 60.0,
        model_id: str = "tiiuae/falcon3-10b-instruct",
        temperature: float = 0.7,
        max_tokens: int = 1024,
        logger=get_logger("general_openai_client"),
        llm_name: str = "general_openai_client"
    ):
        """
        Initialize the OpenAI-compatible client.

        Args:
            api_base (str): API base URL (required)
            api_key (Optional[str]): API key (optional, defaults to None)
            model_id (str): The model ID to use
            temperature (float): The temperature parameter for generation
            max_tokens (int): Maximum number of tokens to generate
            logger (logging.Logger): Logger instance
            llm_name (str): Name of the LLM client for file naming
        """
        # Initialize the parent class
        super().__init__(
            model_id=model_id,
            temperature=temperature,
            max_tokens=max_tokens,
        )

        # Validate required parameters
        if not api_base:
            raise ValueError("API base URL is required")

        # Use default API key if none provided
        if not api_key:
            api_key = "dummy-key"

        self.logger = logger
        self.llm_name = llm_name

        # Initialize the OpenAI client with explicit headers
        self.client = OpenAI(
            api_key=api_key,
            base_url=api_base,
            max_retries=max_retries,
            timeout=timeout,
            default_headers={
                "Content-Type": "application/json",
            }
        )

        # Initialize the AsyncOpenAI client for streaming
        self.async_client = AsyncOpenAI(
            api_key=api_key,
            base_url=api_base,
            max_retries=max_retries,
            timeout=timeout,
            default_headers={
                "Content-Type": "application/json",
            }
        )

        # Store model ID for reference
        self.model_id = model_id
        self.logger.debug(
            f"Initialized OpenAI-compatible client with model: {model_id}")

    # another level of retry, this wait time is increased exponentially
    @retry(max_retries=8, retry_on=(APIError, APIConnectionError, RateLimitError))
    def complete(self, prompt: str) -> str:
        """
        Generate a text completion for the given prompt.
        """
        # Start timing the API request
        start_time = time.time()

        try:
            # Send the prompt to the completion model
            response = self.client.completions.create(
                model=self.model_id,
                prompt=prompt+"\n\n",
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            # Extract content from the response
            content = response.choices[0].text

            # Log response time
            response_time = time.time() - start_time
            self.logger.info(
                "Completion API request completed",
                response_time=round(response_time, 3)
            )

            # Try to log token usage if available
            if hasattr(response, "usage") and response.usage:
                self.logger.info(
                    "Token usage",
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                )

            # Create response metadata for saving
            response_metadata = {
                "model": self.model_id,
                "prompt": prompt,
                "response": content,
                "timestamp": datetime.now().isoformat()
            }

            # Save response for reproducibility
            self._save_raw_response(response_metadata)
            self.logger.debug("Response content", content=content)

            return content

        except Exception as e:
            self.logger.error(f"Unexpected error in complete: {str(e)}")
            raise

    # another level of retry, this wait time is increased exponentially
    @retry(max_retries=8, retry_on=(APIError, APIConnectionError, RateLimitError))
    def complete_chat(self, messages: List[Dict[str, str]]) -> Tuple[str, Any]:
        """
        Generate a response for a chat conversation.

        Args:
            messages (List[Dict[str, str]]): A list of message dictionaries, 
                each containing 'role' (system, user, or assistant) and 'content' keys

        Returns:
            Tuple[str, Any]: A tuple containing:
                - content: The generated text content from the model
                - raw_response: The complete API response object
        """
        # Start timing the API request
        start_time = time.time()

        try:
            # Send the message and get the response
            response = self.client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            # Extract content from the response
            content = response.choices[0].message.content

            # Log response time
            response_time = time.time() - start_time
            self.logger.info(
                "API request completed",
                response_time=round(response_time, 3)
            )

            # Try to log token usage if available
            if hasattr(response, "usage") and response.usage:
                self.logger.info(
                    "Token usage",
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                )

            # Create response metadata for saving
            response_metadata = {
                "model": self.model_id,
                "prompt": messages,
                "response": content,
                "timestamp": datetime.now().isoformat()
            }

            # Save response for reproducibility
            self._save_raw_response(response_metadata)
            self.logger.debug("Response content", content=content)

            return content, response

        except Exception as e:
            self.logger.error(f"Unexpected error: {str(e)}")
            raise

    def complete_chat_once(self, message: str, system_message: Optional[str] = None) -> Tuple[str, Any]:
        """
        Generate a response for a chat conversation with a single call.

        Args:
            message (str): A single prompt message
            system_message (Optional[str]): System message to use for this conversation.
                If None, uses a default system message.

        Returns:
            Tuple[str, Any]: A tuple containing:
                - content: The generated text content from the model
                - raw_response: The complete API response object
        """
        # Use provided system message or default to a standard assistant message
        system_message = system_message or "You are an AI assistant that provides clear, concise explanations."

        # Format messages with system message and user prompt
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": message}
        ]

        # Use complete_chat to handle the request
        return self.complete_chat(messages)

    async def complete_chat_streaming(self, messages: List[ChatCompletionMessageParam]) -> AsyncGenerator[CustomChatCompletionChunk, None]:
        """
        Generate a streaming response for a chat conversation using AsyncOpenAI.

        Args:
            messages (List[ChatCompletionMessageParam]): A list of message dictionaries,
                each containing 'role' (system, user, or assistant) and 'content' keys
        """
        # Start timing the API request
        start_time = time.time()

        try:
            # Send the message and get the streaming response
            stream: Any = await self.async_client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            full_content = {"content": "", "reasoning_content": ""}
            async for chunk in stream:
                chunk: CustomChatCompletionChunk = chunk
                self.logger.debug("Received chunk", chunk=chunk)
                yield chunk
                first_choice = chunk.choices[0]
                if first_choice.delta.content:
                    full_content["content"] += first_choice.delta.content
                if first_choice.delta.reasoning_content:
                    full_content["reasoning_content"] += first_choice.delta.reasoning_content
                # """
                # # TODO: here when outputting reasoning, we need to deal with this differently, what does OpenAI do?
                # =None, function_call=None, refusal=None, role=None, tool_calls=None, reasoning_content=' complex'), finish_reason=None, index=0, logprobs=None, matched_stop=None)], created=1758191469, model='Qwen/Qwen3-4B', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)                                                                                           2025-09-18 10:31:09 [info     ] Received chunk                 [general_openai_client] chunk=ChatCompletionChunk(id='2ab21e6911f243a1b2139a838b6ae8bb', choices=[Choice(delta=ChoiceDelta(content
                # """
                # content_chunk = chunk.choices[0].delta.content
                # full_content += content_chunk
                # yield ChatStreamChunk(content=content_chunk, reasoning_content=chunk.choices[0].reasoning_content)
                # yield content_chunk

            # Log response time
            response_time = time.time() - start_time
            self.logger.info(
                "Streaming API request completed",
                response_time=round(response_time, 3)
            )

            # Create response metadata for saving
            response_metadata = {
                "model": self.model_id,
                "prompt": messages,
                "response": full_content,
                "timestamp": datetime.now().isoformat(),
                "streaming": True
            }

            # Save response for reproducibility
            self._save_raw_response(response_metadata)
            content_length = len(
                full_content["content"]) + len(full_content["reasoning_content"])
            self.logger.debug("Streaming response completed",
                              content_length=content_length)

        except Exception as e:
            self.logger.error(f"Unexpected error in streaming: {str(e)}")
            raise

    def _save_raw_response(self, response: Dict[str, Any]) -> None:
        """
        Saves the raw API response to a file for reproducibility and backup.

        Args:
            response (Dict[str, Any]): The raw API response
            prompt (str): The prompt that was sent to the API
        """
        try:
            # Create a directory for raw responses if it doesn't exist
            raw_responses_dir = os.path.join(get_data_dir(), "raw_responses")
            os.makedirs(raw_responses_dir, exist_ok=True)

            # Generate a filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.model_id.replace(
                ".", "-").replace(":", "-").replace("/", "-")
            filename = f"{self.llm_name}_response_{model_name}_{timestamp}.json"
            filepath = os.path.join(raw_responses_dir, filename)

            # Save the response with the prompt
            with open(filepath, "w") as f:
                json.dump({
                    "model": self.model_id,
                    "timestamp": timestamp,
                    "response": response
                }, f, indent=2)

            self.logger.debug(f"Raw response saved to {filepath}")
        except Exception as e:
            self.logger.error(f"Failed to save raw response: {str(e)}")


if __name__ == "__main__":
    from dotenv import load_dotenv

    # Load environment variables from .env file
    load_dotenv()

    # Get API key from environment
    api_key = os.environ.get("AI71_API_KEY", "")

    # Create a GeneralOpenAIClient instance
    client = GeneralOpenAIClient(
        api_key=api_key,
        api_base="https://api.ai71.ai/v1/",
        model_id="tiiuae/falcon3-10b-instruct"
    )

    # Send the query and get the response with a custom system message
    content, raw_response = client.complete_chat_once(
        "What is retrieval-augmented generation (RAG)?",
        system_message="You are an AI assistant that provides clear, concise explanations."
    )

    # Print the response content
    print("\nResponse from API:")
    print("-" * 50)
    print(content)
    print("-" * 50)

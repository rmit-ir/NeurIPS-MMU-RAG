# /// script
# dependencies = [
#   "spacy",
#   "pandas",
#   "torch",
#   "transformers",
#   "joblib",
#   "nltk",
#   "scikit-learn",
#   "tqdm",
#   "pip",
# ]
# ///
# -*- coding: utf-8 -*-
"""QueryDecompose_LR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M6JYNv2lp8KnaCFUaoEfrg6qiTUL7V0F

Output:

(mmu-rag) ubuntu@ip-172-31-4-11:~/NeurIPS-MMU-RAG$ uv run src/tools/classifiers/QDecompose_V2_Lida.py 
Installed 1 package in 14ms
Loading training file: data/tmp/common4RAGQueryAnalyzer_train.csv
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 175850/175850 [03:50<00:00, 761.34it/s]
Collecting en-core-web-sm==3.8.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 140.4 MB/s  0:00:00
Installing collected packages: en-core-web-sm
Successfully installed en-core-web-sm-3.8.0
✔ Download and installation successful
You can now load the package via spacy.load('en_core_web_sm')
Logistic Regression Accuracy: 0.973841342052886
Logistic Regression f1 Score: 0.9737772203853609
Loading test file: data/sample-test-col-queries/samples_with_labels.csv
Results saved to data/tmp/QDecompose_Lida_output.csv
Logistic Regression Coefficients:

Features sorted by importance (absolute coefficient value):
BERT dim:42, Coefficient: -3.0307
BERT dim:112, Coefficient: 2.5617
X, Coefficient: -2.4784
BERT dim:95, Coefficient: 2.3423
BERT dim:21, Coefficient: 2.3100
BERT dim:17, Coefficient: 2.2504
BERT dim:62, Coefficient: -2.0914
BERT dim:11, Coefficient: -2.0821
BERT dim:69, Coefficient: -1.9802
BERT dim:49, Coefficient: 1.9654
PUNCT, Coefficient: 1.9276
BERT dim:53, Coefficient: -1.9127
BERT dim:86, Coefficient: 1.8874
CCONJ, Coefficient: 1.8184
BERT dim:99, Coefficient: 1.6970
BERT dim:125, Coefficient: 1.6897
BERT dim:61, Coefficient: 1.6658
BERT dim:10, Coefficient: -1.5347
BERT dim:114, Coefficient: 1.5273
BERT dim:109, Coefficient: -1.4803
BERT dim:79, Coefficient: 1.4492
BERT dim:82, Coefficient: 1.4385
BERT dim:110, Coefficient: 1.4222
BERT dim:36, Coefficient: -1.4077
BERT dim:40, Coefficient: 1.4075
BERT dim:23, Coefficient: 1.4034
BERT dim:58, Coefficient: -1.3934
BERT dim:0, Coefficient: -1.3924
BERT dim:93, Coefficient: 1.3721
BERT dim:121, Coefficient: 1.3705
BERT dim:83, Coefficient: 1.3673
BERT dim:107, Coefficient: -1.3424
BERT dim:120, Coefficient: -1.3420
BERT dim:31, Coefficient: 1.3090
BERT dim:44, Coefficient: 1.2484
BERT dim:57, Coefficient: -1.2277
BERT dim:51, Coefficient: -1.2150
BERT dim:37, Coefficient: 1.2012
BERT dim:84, Coefficient: 1.1990
BERT dim:3, Coefficient: -1.1989
BERT dim:18, Coefficient: 1.1897
BERT dim:50, Coefficient: -1.1506
BERT dim:25, Coefficient: 1.1394
BERT dim:13, Coefficient: -1.1365
BERT dim:47, Coefficient: 1.1326
BERT dim:124, Coefficient: -1.1256
BERT dim:94, Coefficient: 1.1242
PROPN, Coefficient: -1.0667
BERT dim:96, Coefficient: -1.0643
NUM, Coefficient: -1.0559
BERT dim:28, Coefficient: 1.0471
BERT dim:54, Coefficient: 1.0233
BERT dim:33, Coefficient: 0.9867
BERT dim:12, Coefficient: -0.9816
BERT dim:108, Coefficient: -0.9700
BERT dim:100, Coefficient: 0.9609
BERT dim:1, Coefficient: 0.9475
BERT dim:65, Coefficient: 0.9467
BERT dim:119, Coefficient: -0.9349
BERT dim:111, Coefficient: 0.9140
BERT dim:76, Coefficient: 0.8969
INTJ, Coefficient: -0.8847
BERT dim:68, Coefficient: 0.8797
BERT dim:2, Coefficient: -0.8769
BERT dim:128, Coefficient: -0.8643
BERT dim:98, Coefficient: 0.8635
NOUN, Coefficient: -0.8625
BERT dim:52, Coefficient: -0.8546
BERT dim:32, Coefficient: 0.8338
BERT dim:87, Coefficient: 0.8237
BERT dim:90, Coefficient: -0.7920
BERT dim:30, Coefficient: 0.7821
BERT dim:5, Coefficient: -0.7763
BERT dim:89, Coefficient: -0.7315
AUX, Coefficient: -0.7183
SCONJ, Coefficient: 0.6936
VERB, Coefficient: -0.6849
BERT dim:70, Coefficient: 0.6847
BERT dim:127, Coefficient: -0.6736
BERT dim:105, Coefficient: 0.6701
BERT dim:46, Coefficient: 0.6571
BERT dim:41, Coefficient: 0.6466
BERT dim:19, Coefficient: -0.6443
BERT dim:126, Coefficient: -0.6298
BERT dim:4, Coefficient: 0.6249
BERT dim:103, Coefficient: -0.6045
BERT dim:91, Coefficient: 0.5809
BERT dim:9, Coefficient: 0.5660
BERT dim:123, Coefficient: -0.5658
BERT dim:72, Coefficient: -0.5512
BERT dim:81, Coefficient: -0.5396
BERT dim:117, Coefficient: -0.5356
BERT dim:8, Coefficient: -0.5336
BERT dim:6, Coefficient: -0.5238
BERT dim:43, Coefficient: -0.5075
BERT dim:45, Coefficient: -0.5052
BERT dim:97, Coefficient: 0.4896
PRON, Coefficient: -0.4847
DET, Coefficient: 0.4575
BERT dim:116, Coefficient: 0.4385
BERT dim:15, Coefficient: 0.4344
BERT dim:66, Coefficient: -0.4341
BERT dim:75, Coefficient: 0.4265
BERT dim:92, Coefficient: -0.4224
SYM, Coefficient: -0.3773
BERT dim:29, Coefficient: -0.3768
BERT dim:122, Coefficient: -0.3629
BERT dim:38, Coefficient: 0.3594
BERT dim:106, Coefficient: -0.3422
BERT dim:22, Coefficient: -0.3382
BERT dim:78, Coefficient: 0.3074
BERT dim:7, Coefficient: 0.3053
BERT dim:60, Coefficient: 0.2844
BERT dim:74, Coefficient: 0.2730
BERT dim:88, Coefficient: 0.2469
BERT dim:64, Coefficient: 0.2355
ADP, Coefficient: 0.2349
BERT dim:16, Coefficient: -0.2127
BERT dim:118, Coefficient: -0.2120
BERT dim:55, Coefficient: -0.2086
BERT dim:34, Coefficient: -0.1981
BERT dim:63, Coefficient: 0.1899
BERT dim:35, Coefficient: -0.1845
BERT dim:115, Coefficient: 0.1770
BERT dim:113, Coefficient: 0.1704
SPACE, Coefficient: 0.1700
BERT dim:73, Coefficient: 0.1603
BERT dim:24, Coefficient: 0.1527
BERT dim:71, Coefficient: -0.1516
BERT dim:80, Coefficient: 0.1430
BERT dim:102, Coefficient: -0.1385
BERT dim:85, Coefficient: -0.1381
BERT dim:48, Coefficient: 0.1334
ADV, Coefficient: -0.1268
BERT dim:20, Coefficient: -0.1264
PART, Coefficient: 0.1246
BERT dim:77, Coefficient: -0.1205
BERT dim:67, Coefficient: 0.0958
BERT dim:27, Coefficient: 0.0956
BERT dim:39, Coefficient: -0.0868
BERT dim:56, Coefficient: 0.0721
BERT dim:104, Coefficient: 0.0602
BERT dim:101, Coefficient: 0.0556
BERT dim:14, Coefficient: 0.0385
BERT dim:26, Coefficient: -0.0165
BERT dim:59, Coefficient: -0.0017
CONJ, Coefficient: 0.0000

Model saved to qdecompose_model.joblib
Model size: 3,604 bytes (0.00 MB)
(mmu-rag) ubuntu@ip-172-31-4-11:~/NeurIPS-MMU-RAG$ 
"""

import spacy
import pandas as pd
import torch
import copy
import numpy as np
from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel
import joblib
import nltk
nltk.download('punkt', quiet=True) # For tokenization
nltk.download('averaged_perceptron_tagger', quiet=True) # For POS tagging

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

model_key = "google/bert_uncased_L-2_H-128_A-2"

tokenizer = AutoTokenizer.from_pretrained(model_key)
model = AutoModel.from_pretrained(model_key)

filename = 'data/tmp/common4RAGQueryAnalyzer_train.csv'
print(f'Loading training file: {filename}')

## Reading the csv file into a dataframe and converting YES/NO to 1/0 calss labels
df = pd.read_csv(filename)
df.head()
df['class'] = df['class'].\
    apply(lambda x: 0 if x == 'NO' else 1)
df.head()

## function that receives a query as an inout and produces tiny bert embeddings with 128 dimensions
def get_bert_embedding(text):
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)
        with torch.no_grad():
            outputs = model(**inputs)
        # Extract the [CLS] token embedding (first token)
        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()
        return cls_embedding.numpy()

    # Example usage

## preparing the X and Y for Logistic Regression with X being the 128 dimension BERT embedding
Y = list(df['class'])
from tqdm import tqdm
X = []
for text in tqdm(list(df['question'])):
  X.append(get_bert_embedding(text))

# Load the English language model
# download spacy download en_core_web_lg
spacy.cli.download("en_core_web_sm")
nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])

# POS Tag Labels that we are interested in and are produces by Spacy
POS_LIST = ["ADJ", "ADP", "ADV", "AUX", "CONJ", "CCONJ", "DET", "INTJ", "NOUN", "NUM", "PART", "PRON", "PROPN", "PUNCT", "SCONJ", "SYM", "VERB", "X", "SPACE"]


## Creating an augmented list for additional engineered features
embedding_dim = len(X[0])
augmented_data = copy.deepcopy(X)
tag_labels = list(POS_LIST)


# Reading the queries and providing POS tags, i.e., engineered features, each representing the number of POS tags we discover in the query, e.g., 2 Nouns.
sentences = list(df['question'])
for index in range(len(augmented_data)):
  augmented_data[index] = np.concatenate((augmented_data[index],np.zeros(len(tag_labels))))

index = 0
for doc in nlp.pipe(sentences):
  for token in doc:
    augmented_data[index][embedding_dim+tag_labels.index(token.pos_)]+=1
  index+=1

# training and testing split for the augmented data with probabilities of class 1 being in prob_class_1 variable
X_train, X_test, y_train, y_test = train_test_split(augmented_data, Y, test_size=0.2, random_state=42)

log_reg_model = LogisticRegression(max_iter=1000) # Increase max_iter for convergence
log_reg_model.fit(X_train, y_train)

y_pred = log_reg_model.predict(X_test)

probabilities = log_reg_model.predict_proba(X_test)
prob_class_1 = probabilities[:, 1]
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Logistic Regression Accuracy: {accuracy}")
print(f"Logistic Regression f1 Score: {f1}")

# Load test data from local file (if it exists)
test_filename = 'data/sample-test-col-queries/samples_with_labels.csv'
print(f'Loading test file: {test_filename}')

# reading the test csv file and getting the BERT embedding stored in test_unseen
df_test = pd.read_csv(test_filename)
df_test.head()
test_unseen =list([get_bert_embedding(text) for text in list(df_test['question'])])

## Reading the queries and providing POS tags, i.e., engineered features, each representing the number of POS tags we discover in the query, e.g., 2 Nouns.
augmented_test_data = copy.deepcopy(test_unseen)

test_sentences = list(df_test['question'])
for index in range(len(augmented_test_data)):
  augmented_test_data[index] = np.concatenate((augmented_test_data[index],np.zeros(len(tag_labels))))

index = 0
for doc in nlp.pipe(test_sentences):
  for token in doc:
    augmented_test_data[index][embedding_dim+tag_labels.index(token.pos_)]+=1
  index+=1

# testing the learned logisitic regression model on the augmented test data
probabilities = log_reg_model.predict_proba(augmented_test_data)
prob_class_1 = probabilities[:, 1]

rounded_list = [ round(elem, 4) for elem in prob_class_1 ]

df_test['prob'] = rounded_list
df_test.to_csv('data/tmp/QDecompose_Lida_output.csv', index=False)
print('Results saved to data/tmp/QDecompose_Lida_output.csv')

# Further Analysis of the coefficients, did the POS tags help?
print("Logistic Regression Coefficients:")

coefficients = log_reg_model.coef_[0]  # For binary classification, it's typically a 1D array
intercept = log_reg_model.intercept_[0]

sorted_indices = np.argsort(np.abs(coefficients))[::-1]
print("\nFeatures sorted by importance (absolute coefficient value):")
for i in sorted_indices:
  if i>128:
    print(f"{POS_LIST[i-128]}, Coefficient: {coefficients[i]:.4f}")
  else:
    print(f"BERT dim:{i}, Coefficient: {coefficients[i]:.4f}")

# Save the trained model using joblib
import os
import joblib

to_model_path = "models/qdecompose_model.joblib"

model_data = {
    'log_reg_model': log_reg_model,
    'model_key': model_key,
    'tokenizer_key': model_key,
    'embedding_dim': embedding_dim,
    'POS_LIST': POS_LIST,
    'coefficients': coefficients,
    'intercept': intercept
}
joblib.dump(model_data, to_model_path)

# Get and display the size of the saved model
model_size = os.path.getsize(to_model_path)
model_size_mb = model_size / (1024 * 1024)
print(f'\nModel saved to {to_model_path}')
print(f'Model size: {model_size:,} bytes ({model_size_mb:.2f} MB)')

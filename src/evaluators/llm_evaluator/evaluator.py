"""
LLM-based evaluator for RAG system assessment.

This module implements an evaluator that uses Claude Sonnet 3.5 through the Bedrock client
to assess the quality of responses generated by a RAG system based on relevance and faithfulness.

uv run scripts/evaluate.py --evaluator evaluators.llm_evaluator.llm_evaluator.LLMEvaluator --help
"""
from typing import List, Dict, Any, Optional
import json
import re
import time
from statistics import mean
import concurrent.futures
from threading import Lock

from src.evaluators.evaluator_interface import EvaluatorInterface, EvaluationResult
import os
from evaluators.llm_evaluator.prompts import (
    SUPPORT_PROMPT_TEMPLATE,
    SYSTEM_PROMPT_CORRECTNESS,
    CORRECTNESS_PROMPT_TEMPLATE,
    SUMMARY_SYSTEM_PROMPT,
    SUMMARY_PROMPT_TEMPLATE,
    SYSTEM_PROMPT_SUPPORT
)
from evaluators.llm_evaluator.prompts_combined import (
    SYSTEM_PROMPT,
    SUMMARY_SYSTEM_PROMPT,
    SUMMARY_PROMPT_TEMPLATE
)
import openai
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMEvaluator(EvaluatorInterface):
    """
    LLM-based evaluator for RAG system assessment.

    This evaluator uses LLM through the MMU PROXY Router server to assess
    the quality of responses generated by a RAG system based on:

    1. Relevance - Measures the correctness and relevance of the answer to the question
       on a four-point scale:
       2: The response correctly answers the user question and contains no irrelevant content
       1: The response provides a useful answer to the user question, but may contain irrelevant
          content that do not harm the usefulness of the answer
       0: No answer is provided in the response (e.g., "I don't know")
       -1: The response does not answer the question whatsoever

    2. Faithfulness - Assess whether the response is grounded in the retrieved passages
       on a three-point scale:
       1: Full support, all answer parts are grounded
       0: Partial support, not all answer parts are grounded
       -1: No support, all answer parts are not grounded
    """
    def __init__(
        self,
        model_id: str = "openai.gpt-oss-20b-1:0",
        temperature: float = 0.0,
        max_tokens: int = 2048,
        use_gold_references: bool = True,
        silent_errors: bool = True,
        num_threads: int = 1,
        perform_system_analysis: bool = True,
        skip_final_summary: bool = True,
        num_samples_for_analysis: int = 40,
        answer_word_limit = 300,
        api_base: str = "https://mmu-proxy-server-llm-proxy.rankun.org",
        api_key: Optional[str] = None,
        combined_prompt: bool = True
    ):
        """
        Initialize the LLM evaluator.

        Args:
            model_id (str): The model ID to use for evaluation.
            temperature (float): The temperature setting for the LLM.
            max_tokens (int): The maximum number of tokens to generate.
            use_gold_references (bool): Whether to use gold reference answers if available.
            silent_errors (bool): If True, log errors and continue; if False, raise exceptions.
            num_threads (int): Number of threads to use for concurrent evaluations.
            perform_system_analysis (bool): Whether to perform system-level analysis after evaluations.
            skip_final_summary (bool): Whether to skip generating a final summary of results.
            num_samples_for_analysis (int): Number of samples to use for system analysis.
            answer_word_limit (int): Maximum word limit for answers in prompts.
            api_base (str): Base URL for the MMU PROXY Router server.
            api_key (Optional[str]): API key for authentication with the MMU PROXY Router server.
            combined_prompt (bool): Whether to use the combined prompt for both relevance and faithfulness.
        """
        self.model_id = model_id
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.use_gold_references = use_gold_references
        self.silent_errors = silent_errors
        self.num_threads = num_threads
        self.perform_system_analysis = perform_system_analysis
        self.skip_final_summary = skip_final_summary
        self.num_samples_for_analysis = num_samples_for_analysis
        self.answer_word_limit = answer_word_limit
        self.api_base = api_base
        self.api_key = api_key

        self.combined_prompt = combined_prompt

        # Initialize OpenAI client
        self.client = openai.OpenAI(
            base_url=self.api_base,
            api_key=self.api_key or os.getenv("MMU_OPENAI_API_KEY")
        )
        if not self.client.api_key:
            raise ValueError("API key for MMU PROXY Router server is not set. Please provide it via 'api_key' or 'MMU_OPENAI_API_KEY' environment variable.")
        

    def _create_evaluation_prompt(self, question: str, answer: str, documents: List[str], gold_reference: Optional[str] = None) -> str:
        """
        Create the evaluation prompt for a single sample.
        """
        # Format the documents
        documents_text = ""
        for i, doc in enumerate(documents, 1):
            documents_text += f"\nDocument {i}:\n{doc}\n"

        # Format the gold reference section
        gold_ref_text = ""
        if self.use_gold_references and gold_reference:
            gold_ref_text = f"""GOLD REFERENCE ANSWER:
{gold_reference}

"""

        # Choose prompt template based on combined_prompt flag
        if self.combined_prompt:
            from evaluators.llm_evaluator.prompts_combined import EVALUATION_PROMPT_TEMPLATE
            prompt_template = EVALUATION_PROMPT_TEMPLATE
        else:
            # For separate prompts, we'll handle this in _evaluate_single
            return ""

        # Fill in the template
        prompt = prompt_template.format(
            question=question,
            answer=answer[:self.answer_word_limit*10] if self.answer_word_limit else answer,  # Rough word limit
            gold_reference=gold_ref_text,
            documents=documents_text
        )

        return prompt

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        Parse the LLM response to extract evaluation scores.
        """
        # Try to extract JSON using regex
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # If no code block, try to find JSON directly
            json_match = re.search(r'(\{.*\})', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                logger.error(f"Failed to extract JSON from LLM response: {response}")
                return {
                    "relevance_score": 0,
                    "faithfulness_score": 0,
                    "evaluation_notes": "Failed to parse LLM response"
                }

        try:
            result = json.loads(json_str)
            return result
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            return {
                "relevance_score": 0,
                "faithfulness_score": 0,
                "evaluation_notes": f"JSON parsing error: {str(e)}"
            }

    def _evaluate_single(self, system_output: Dict[str, Any], reference: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate a single system output.
        """
        try:
            question = system_output.get('query', '')
            answer = system_output.get('generated_response', system_output.get('answer', ''))
            documents = system_output.get('contexts', system_output.get('documents', []))
            
            # Debug logging
            logger.info(f"Evaluating sample - question: {question[:50]}..., answer length: {len(answer)}, documents: {len(documents)}")
            
            # Get gold reference if available
            gold_reference = None
            if self.use_gold_references and reference:
                gold_reference = reference.get('reference', reference.get('answer', ''))
                logger.info(f"Gold reference available, length: {len(gold_reference) if gold_reference else 0}")

            if self.combined_prompt:
                # Use combined prompt
                from evaluators.llm_evaluator.prompts_combined import SYSTEM_PROMPT, EVALUATION_PROMPT_TEMPLATE
                
                prompt = self._create_evaluation_prompt(question, answer, documents, gold_reference)
                logger.info(f"Created prompt, length: {len(prompt)}")
                
                response = self.client.chat.completions.create(
                    model=self.model_id,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                llm_response = response.choices[0].message.content
                logger.info(f"LLM response length: {len(llm_response)}, content: {llm_response[:200]}...")
                evaluation = self._parse_llm_response(llm_response)
                
            else:
                # Use separate prompts for relevance and faithfulness
                from evaluators.llm_evaluator.prompts import (
                    SYSTEM_PROMPT_CORRECTNESS, CORRECTNESS_PROMPT_TEMPLATE,
                    SYSTEM_PROMPT_SUPPORT, SUPPORT_PROMPT_TEMPLATE
                )
                
                # Evaluate relevance
                relevance_prompt = self._create_evaluation_prompt(question, answer, documents, gold_reference)
                relevance_prompt = CORRECTNESS_PROMPT_TEMPLATE.format(
                    question=question,
                    answer=answer[:self.answer_word_limit*10] if self.answer_word_limit else answer,
                    gold_reference=gold_reference or "",
                    documents=self._create_evaluation_prompt(question, answer, documents, gold_reference).split('RETRIEVED DOCUMENTS:')[1] if 'RETRIEVED DOCUMENTS:' in self._create_evaluation_prompt(question, answer, documents, gold_reference) else ""
                )
                
                relevance_response = self.client.chat.completions.create(
                    model=self.model_id,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT_CORRECTNESS},
                        {"role": "user", "content": relevance_prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                relevance_eval = self._parse_llm_response(relevance_response.choices[0].message.content)
                
                # Evaluate faithfulness
                faithfulness_prompt = SUPPORT_PROMPT_TEMPLATE.format(
                    question=question,
                    answer=answer[:self.answer_word_limit*10] if self.answer_word_limit else answer,
                    gold_reference=gold_reference or "",
                    documents=self._create_evaluation_prompt(question, answer, documents, gold_reference).split('RETRIEVED DOCUMENTS:')[1] if 'RETRIEVED DOCUMENTS:' in self._create_evaluation_prompt(question, answer, documents, gold_reference) else ""
                )
                
                faithfulness_response = self.client.chat.completions.create(
                    model=self.model_id,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT_SUPPORT},
                        {"role": "user", "content": faithfulness_prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                faithfulness_eval = self._parse_llm_response(faithfulness_response.choices[0].message.content)
                
                # Combine results
                evaluation = {
                    'evaluation_notes': f"{relevance_eval.get('evaluation_notes', '')}\n\n{faithfulness_eval.get('evaluation_notes', '')}",
                    'relevance_score': relevance_eval.get("relevance_score", 0),
                    'faithfulness_score': faithfulness_eval.get("faithfulness_score", 0)
                }

            return evaluation
            
        except Exception as e:
            if self.silent_errors:
                logger.error(f"Error during evaluation: {e}")
                return {
                    "relevance_score": 0,
                    "faithfulness_score": 0,
                    "evaluation_notes": f"Evaluation error: {str(e)}"
                }
            else:
                raise e

    def evaluate(self, system_outputs: List[Dict[str, Any]], references: List[Dict[str, Any]]) -> EvaluationResult:
        """
        Evaluate system outputs against references.
        """
        start_time = time.time()
        
        # Validate inputs
        self.validate_inputs(system_outputs, references)
        
        # Create lookup for references
        ref_lookup = {ref.get('iid', ref.get('query_id')): ref for ref in references}
        
        rows = []
        relevance_scores = []
        faithfulness_scores = []
        
        def evaluate_sample(output, reference):
            result = self._evaluate_single(output, reference)
            rows.append({
                'query_id': output.get('iid', output.get('query_id')),
                'relevance_score': result.get('relevance_score', 0),
                'faithfulness_score': result.get('faithfulness_score', 0),
                'evaluation_notes': result.get('evaluation_notes', '')
            })
            relevance_scores.append(result.get('relevance_score', 0))
            faithfulness_scores.append(result.get('faithfulness_score', 0))
        
        if self.num_threads > 1:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_threads) as executor:
                futures = []
                for output in system_outputs:
                    output_id = output.get('iid', output.get('query_id'))
                    reference = ref_lookup.get(output_id, {})
                    futures.append(executor.submit(evaluate_sample, output, reference))
                
                for future in concurrent.futures.as_completed(futures):
                    future.result()
        else:
            for output in system_outputs:
                output_id = output.get('iid', output.get('query_id'))
                reference = ref_lookup.get(output_id, {})
                evaluate_sample(output, reference)
        
        # Calculate metrics
        avg_relevance = mean(relevance_scores) if relevance_scores else 0.0
        avg_faithfulness = mean(faithfulness_scores) if faithfulness_scores else 0.0
        
        metrics = {
            'avg_relevance_score': avg_relevance,
            'avg_faithfulness_score': avg_faithfulness,
            'count': len(system_outputs)
        }
        
        total_time_ms = (time.time() - start_time) * 1000
        
        return EvaluationResult(
            metrics=metrics,
            evaluator_name=self.name,
            sample_count=len(system_outputs),
            rows=rows,
            total_time_ms=total_time_ms
        )

    @property
    def name(self) -> str:
        """Return evaluator name."""
        return "LLM"

if __name__ == "__main__":
    # Main entry point for testing the evaluator
    evaluator = LLMEvaluator(
        model_id="openai.gpt-oss-20b-1:0",
        temperature=0.0,
        max_tokens=2048,
        use_gold_references=False,
        silent_errors=True,
        num_threads=2,
        perform_system_analysis=False,
        skip_final_summary=True,
        num_samples_for_analysis=40,
        answer_word_limit=300,
        api_base="https://mmu-proxy-server-llm-proxy.rankun.org",
        api_key=os.getenv("MMU_OPENAI_API_KEY"),
        combined_prompt=True
    )
    
    question = "What is retrieval-augmented generation?"
    answer = "Retrieval-augmented generation (RAG) is a technique that combines retrieval of relevant documents with text generation."
    context = [
        "Retrieval-augmented generation (RAG) is an AI framework that combines information retrieval with text generation."]
    doc_ids = ["doc1"]
    total_time_ms = 100.0
    qid = "1"
    system_name = "TestRAGSystem"
    refquestion = "What is retrieval-augmented generation?"
    refanswer = "Retrieval-augmented generation (RAG) is an AI framework that enhances large language models by retrieving external knowledge."

    logger.info("Starting evaluation...")
    result = evaluator.evaluate(
        system_outputs=[
            {
                'iid': '1',
                'query': question,
                'answer': answer,
                'documents': context,
                'document_ids': doc_ids,
                'total_time_ms': total_time_ms,
                'system_name': system_name
            }
        ],
        references=[
            {
                'iid': '1',
                'query': refquestion,
                'answer': refanswer
            }
        ]
    )
    logger.info(f"Evaluation result: {result.metrics}")

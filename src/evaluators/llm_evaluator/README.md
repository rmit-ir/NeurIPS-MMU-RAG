# LLM Evaluator

## Overview

The LLM Evaluator is a sophisticated evaluation tool that uses large language models to assess the quality of responses generated by Retrieval-Augmented Generation (RAG) systems. It provides detailed, human-like evaluation of response relevance and faithfulness to source documents.

**Path:** `src.evaluators.llm_evaluator.evaluator.LLMEvaluator`

## Evaluation Metrics

### Relevance Score (4-point scale)
- **2**: The response correctly answers the user question and contains no irrelevant content
- **1**: The response provides a useful answer to the user question, but may contain irrelevant content that does not harm the usefulness of the answer
- **0**: No answer is provided in the response (e.g., "I don't know")
- **-1**: The response does not answer the question whatsoever

### Faithfulness Score (3-point scale)
- **1**: Full support - all answer parts are grounded in the retrieved documents
- **0**: Partial support - not all answer parts are grounded in the retrieved documents
- **-1**: No support - all answer parts are not grounded in the retrieved documents

## Key Features

- **LLM-powered Assessment**: Uses Claude Sonnet 3.5 through the MMU PROXY Router server for intelligent evaluation
- **Detailed Explanations**: Provides evaluation notes explaining the reasoning behind each score
- **Gold Reference Integration**: Optionally incorporates gold reference answers for enhanced evaluation accuracy
- **Concurrent Processing**: Supports multi-threaded evaluation for improved performance
- **Flexible Configuration**: Extensive parameters for customizing evaluation behavior
- **Combined Prompt System**: Uses integrated prompts for both relevance and faithfulness assessment

## Parameters

### Core Parameters
- `model_id` (str): The model ID to use for evaluation (default: "openai.gpt-oss-20b-1:0")
- `temperature` (float): The temperature setting for the LLM (default: 0.0)
- `max_tokens` (int): The maximum number of tokens to generate (default: 2048)

### Data Processing
- `use_gold_references` (bool): Whether to use gold reference answers if available (default: True)
- `answer_word_limit` (int): Maximum word limit for answers in prompts (default: 300)
- `combined_prompt` (bool): Whether to use the combined prompt for both metrics (default: True)

### Performance & Reliability
- `num_threads` (int): Number of threads to use for concurrent evaluations (default: 1)
- `silent_errors` (bool): If True, log errors and continue; if False, raise exceptions (default: True)

### Analysis Features
- `perform_system_analysis` (bool): Whether to perform system-level analysis after evaluations (default: True)
- `skip_final_summary` (bool): Whether to skip generating a final summary of results (default: True)
- `num_samples_for_analysis` (int): Number of samples to use for system analysis (default: 40)

### API Configuration
- `api_base` (str): Base URL for the MMU PROXY Router server (default: "https://mmu-proxy-server-llm-proxy.rankun.org")
- `api_key` (Optional[str]): API key for authentication (can also use `MMU_OPENAI_API_KEY` environment variable)

## Usage Examples

### Basic Usage

```bash
python scripts/evaluate.py \
    --evaluator LLMEvaluator \
    --results data/system_outputs.jsonl \
    --reference data/references.jsonl \
    --output-dir data/evaluation_results \
    --output-prefix my_evaluation
```

### Advanced Configuration

```bash
python scripts/evaluate.py \
    --evaluator LLMEvaluator \
    --results data/system_outputs.jsonl \
    --reference data/references.jsonl \
    --output-dir data/evaluation_results \
    --output-prefix advanced_eval \
    --model-id "openai.gpt-oss-20b-1:0" \
    --temperature 0.0 \
    --max-tokens 2048 \
    --num-threads 4 \
    --answer-word-limit 500 \
    --use-gold-references \
    --combined-prompt
```

### With Custom API Configuration

```bash
python scripts/evaluate.py \
    --evaluator LLMEvaluator \
    --results data/system_outputs.jsonl \
    --reference data/references.jsonl \
    --output-dir data/evaluation_results \
    --output-prefix custom_api_eval \
    --api-base "https://custom-mmu-proxy.example.com" \
    --api-key "your-api-key-here"
```

## Input Data Format

The evaluator expects JSONL files with the following structure:

### Results File (system outputs)
```json
{
  "query_id": "unique_query_identifier",
  "generated_response": "The RAG system's response text",
  "contexts": ["document1", "document2", "..."]
}
```

### Reference File (gold answers)
```json
{
  "query_id": "unique_query_identifier",
  "answer": "Gold reference answer text"
}
```

## Output Format

The evaluator produces two output files in the specified output directory:

### Row-level Results (`{prefix}.rows.jsonl`)
```json
{
  "query_id": "V_0522",
  "relevance_score": 2,
  "faithfulness_score": 1,
  "evaluation_notes": "Detailed explanation of the evaluation reasoning..."
}
```

### Aggregated Results (`{prefix}.aggregated.jsonl`)
```json
{
  "metrics": {
    "avg_relevance_score": 1.5,
    "avg_faithfulness_score": 0.8,
    "count": 100
  },
  "evaluator_name": "LLM",
  "sample_count": 100,
  "timestamp": "2025-10-09T12:00:00.000000",
  "total_time_ms": 45000.0
}
```

## Performance Considerations

- **Concurrent Processing**: Use `num_threads > 1` for faster evaluation of large datasets
- **Token Limits**: Adjust `max_tokens` based on the complexity of your evaluation prompts
- **Answer Length**: The `answer_word_limit` parameter helps manage prompt size for very long responses
- **API Rate Limits**: The evaluator includes built-in error handling for API limitations

## Troubleshooting

### Common Issues

1. **Empty Response Detection**: If scores are consistently 0/-1, check that the `generated_response` field in your results file contains actual text content.

2. **API Connection Errors**: Verify that `api_base` and `api_key` are correctly configured, or that the `MMU_OPENAI_API_KEY` environment variable is set.

3. **Memory Issues**: For large datasets, reduce `num_threads` or process data in smaller batches.

4. **Long Evaluation Times**: Increase `num_threads` for parallel processing, or reduce `max_tokens` if detailed explanations aren't needed.

### Debug Logging

Enable detailed logging to troubleshoot issues:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Integration with Other Evaluators

The LLM Evaluator can be used alongside other evaluators for comprehensive RAG assessment:

```bash
# Run multiple evaluators
for evaluator in ["LLMEvaluator", "DeepEvalEvaluator"]; do
    python scripts/evaluate.py \
        --evaluator $evaluator \
        --results data/system_outputs.jsonl \
        --reference data/references.jsonl \
        --output-dir data/evaluation_results \
        --output-prefix ${evaluator}_eval
done
```

Or run them individually:

```bash
# LLM evaluation
python scripts/evaluate.py \
    --evaluator LLMEvaluator \
    --results data/system_outputs.jsonl \
    --reference data/references.jsonl \
    --output-dir data/evaluation_results \
    --output-prefix llm_eval

# DeepEval evaluation
python scripts/evaluate.py \
    --evaluator DeepEvalEvaluator \
    --results data/system_outputs.jsonl \
    --reference data/references.jsonl \
    --output-dir data/evaluation_results \
    --output-prefix deepeval_eval

# RAGAS evaluation
python scripts/evaluate.py \
    --evaluator RAGASEvaluator \
    --results data/system_outputs.jsonl \
    --reference data/references.jsonl \
    --output-dir data/evaluation_results \
    --output-prefix ragas_eval
```

## Dependencies

- `openai` (v1.0+): For API communication with the LLM service
- `concurrent.futures`: For multi-threaded evaluation
- `statistics`: For metric aggregation
- Standard library modules: `json`, `re`, `time`, `logging`

## File Structure

```
src/evaluators/llm_evaluator/
├── __init__.py
├── evaluator.py              # Main evaluator implementation
├── prompts.py                # Individual prompt templates
├── prompts_combined.py       # Combined prompt templates
└── README.md                 # This documentation
```</content>
<filePath">C:\Users\halfull\Documents\NeurIPS-MMU-RAG\src\evaluators\llm_evaluator\README.md